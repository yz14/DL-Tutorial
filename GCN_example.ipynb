{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599703943031",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Networks. See this [Paper][https://arxiv.org/abs/1609.02907], [Thomas Kipf's Github][https://github.com/tkipf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "path = 'D:/Code/Graph/GCN/py_gcn/data/cora/'\n",
    "dataset = 'cora'\n",
    "n_hid = 16\n",
    "n_class = 7\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "seed = 42\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "dropout = 0.5\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data  \n",
    "* Adjacency matrix: $G \\in R^{n\\times n}$  \n",
    "* Node features: $X \\in R^{n\\times p}$  \n",
    "* Node labels: $t \\in R^{n}$  \n",
    "### 1.1 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    # CSR to COO\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def build_graph(idx, edges):\n",
    "    # node index range: 0 -> n-1\n",
    "    # rename node index and edges\n",
    "    idx_dict = {i:j for j,i in enumerate(idx)}\n",
    "    n = len(idx_dict) # num_node\n",
    "    re_edges = list(map(idx_dict.get, edges.reshape(-1)))\n",
    "    re_edges = np.array(re_edges).reshape(edges.shape)\n",
    "    # adjacency (sparse matrix)\n",
    "    values = np.ones(re_edges.shape[0])\n",
    "    v_i, v_j = re_edges[:,0], re_edges[:,1]\n",
    "    G = sp.coo_matrix((values, (v_i, v_j)), shape=(n,n))\n",
    "    # symmetric adjacency\n",
    "    mask = (G + G.T)>=1\n",
    "    G = mask*1. + sp.eye(mask.shape[0])\n",
    "    # row normalization\n",
    "    G = normalization(G)\n",
    "    # numpy -> tensor sparse matrix\n",
    "    G = sparse_mx_to_torch_sparse_tensor(G)\n",
    "    return G\n",
    "\n",
    "def class_name2labels(name):\n",
    "    '''Convert class names to digits'''\n",
    "    class_name = set(name)\n",
    "    class_dict = {n:i for i,n in enumerate(class_name)}\n",
    "    labels = torch.LongTensor(list(map(class_dict.get, name)))\n",
    "    return labels\n",
    "\n",
    "def load_data(path, dataset):\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\\\n",
    "        dtype=np.dtype(str))\n",
    "    edges = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\\\n",
    "        dtype=np.int32) # shape: n_edges*2\n",
    "    # node index\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    # Adjacency\n",
    "    G = build_graph(idx, edges)\n",
    "    # node features\n",
    "    X = np.array(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    X = torch.FloatTensor(normalization(X))\n",
    "    # node labels\n",
    "    t = class_name2labels(idx_features_labels[:, -1])\n",
    "    t = torch.LongTensor(t)\n",
    "    # train, val, test indexs\n",
    "    idx_total = np.arange(t.shape[0])\n",
    "    idx_train, idx_test = train_test_split(idx_total, test_size=0.6, stratify=t)\n",
    "    idx_train, idx_val = train_test_split(idx_train, test_size=0.6, stratify=t[idx_train])\n",
    "    idx_train, idx_none = train_test_split(idx_train, test_size=0.3, stratify=t[idx_train])\n",
    "    idx_train_val_test = [idx_train, idx_val, idx_test]\n",
    "    idx_train, idx_val, idx_test = map(torch.LongTensor,idx_train_val_test)\n",
    "\n",
    "    return G,X,t,idx_train, idx_val, idx_test\n",
    "\n",
    "def normalization(data, mode='l1'):\n",
    "    norm = Normalizer(norm=mode)\n",
    "    data = norm.fit_transform(data)\n",
    "    return data\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    y = torch.argmax(logits, 1)\n",
    "    num_corrects = (y==labels).sum()\n",
    "    return num_corrects.float() / len(labels)\n",
    "\n",
    "# train\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # forward\n",
    "    model.train() # dropout\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X, G)\n",
    "    loss_train = F.cross_entropy(logits[idx_train], ts[idx_train])\n",
    "    acc_train = accuracy(logits[idx_train], ts[idx_train])\n",
    "    # backward\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval() # no dropout, no grad\n",
    "    logits = model(X, G)\n",
    "\n",
    "    loss_val = F.cross_entropy(logits[idx_val], ts[idx_val])\n",
    "    acc_val = accuracy(logits[idx_val], ts[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "# test\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits = model(X, G)\n",
    "    loss_test = F.cross_entropy(logits[idx_test], ts[idx_test])\n",
    "    acc_test = accuracy(logits[idx_test], ts[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Graph size: torch.Size([2708, 2708])\nFeatures size: torch.Size([2708, 1433])\nlabels: torch.Size([2708])\n"
    }
   ],
   "source": [
    "G, X, ts, idx_train, idx_val, idx_test = load_data(path, dataset)\n",
    "\n",
    "# use GPU\n",
    "X = X.to(device)\n",
    "G = G.to(device)\n",
    "ts = ts.to(device)\n",
    "idx_train = idx_train.to(device)\n",
    "idx_val = idx_val.to(device)\n",
    "idx_test = idx_test.to(device)\n",
    "\n",
    "print(' Graph size: {}\\nFeatures size: {}\\nlabels: {}'\\\n",
    "    .format(G.size(), X.shape, t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Convolutional Network Model  \n",
    "$f = \\sigma(GXW)$  \n",
    "$G$: with self-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class GCN(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GCN,self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Model parameters: weight and bias\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features,out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "        # Parameters initialization\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        nn.init.uniform_(self.weight.data)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # Z = XW\n",
    "        z = torch.mm(x, self.weight)\n",
    "        # Z = AZ\n",
    "        # adj is a sparse tensor: values, v_i, v_j\n",
    "        z = torch.spmm(adj, z)\n",
    "        if self.bias is not None:\n",
    "            z = z+self.bias \n",
    "        return z\n",
    "\n",
    "    def __repr__(self):\n",
    "        info = self.__class__.__name__+\\\n",
    "            '('+str(self.in_features)+'->'+str(self.out_features)+')'\n",
    "        return info\n",
    "\n",
    "class GCN_Model(nn.Module):\n",
    "    # Two layers\n",
    "    def __init__(self, n_in, n_hid, n_class, dropout):\n",
    "        super(GCN_Model, self).__init__()\n",
    "        self.feat = GCN(n_in, n_hid)\n",
    "        self.clf = GCN(n_hid, n_class)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.feat(x,adj)) # 1st layer\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        logits = self.clf(x,adj) # output\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "n_in, n_class = X.size(1), ts.max().item()+1\n",
    "\n",
    "model = GCN_Model(n_in=n_in, n_hid=n_hid, n_class=n_class, dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0001 loss_train: 2.2716 acc_train: 0.1485 loss_val: 2.0153 acc_val: 0.1308 time: 0.4198s\nEpoch: 0002 loss_train: 2.0218 acc_train: 0.1716 loss_val: 1.9432 acc_val: 0.1308 time: 0.0179s\nEpoch: 0003 loss_train: 2.0273 acc_train: 0.2244 loss_val: 1.8937 acc_val: 0.1615 time: 0.0175s\nEpoch: 0004 loss_train: 2.0291 acc_train: 0.1716 loss_val: 1.8618 acc_val: 0.3015 time: 0.0160s\nEpoch: 0005 loss_train: 1.9348 acc_train: 0.2442 loss_val: 1.8414 acc_val: 0.3015 time: 0.0160s\nEpoch: 0006 loss_train: 1.9609 acc_train: 0.2244 loss_val: 1.8287 acc_val: 0.3015 time: 0.0150s\nEpoch: 0007 loss_train: 1.9087 acc_train: 0.2706 loss_val: 1.8209 acc_val: 0.3015 time: 0.0160s\nEpoch: 0008 loss_train: 1.9817 acc_train: 0.2574 loss_val: 1.8165 acc_val: 0.3015 time: 0.0150s\nEpoch: 0009 loss_train: 1.8900 acc_train: 0.2706 loss_val: 1.8143 acc_val: 0.3015 time: 0.0160s\nEpoch: 0010 loss_train: 1.8702 acc_train: 0.3036 loss_val: 1.8135 acc_val: 0.3015 time: 0.0150s\nEpoch: 0011 loss_train: 1.8688 acc_train: 0.2970 loss_val: 1.8132 acc_val: 0.3015 time: 0.0150s\nEpoch: 0012 loss_train: 1.8914 acc_train: 0.2739 loss_val: 1.8131 acc_val: 0.3015 time: 0.0160s\nEpoch: 0013 loss_train: 1.8784 acc_train: 0.2706 loss_val: 1.8127 acc_val: 0.3015 time: 0.0199s\nEpoch: 0014 loss_train: 1.8714 acc_train: 0.2739 loss_val: 1.8117 acc_val: 0.3015 time: 0.0190s\nEpoch: 0015 loss_train: 1.8579 acc_train: 0.3003 loss_val: 1.8099 acc_val: 0.3015 time: 0.0150s\nEpoch: 0016 loss_train: 1.8455 acc_train: 0.2772 loss_val: 1.8074 acc_val: 0.3015 time: 0.0150s\nEpoch: 0017 loss_train: 1.8160 acc_train: 0.2805 loss_val: 1.8043 acc_val: 0.3015 time: 0.0180s\nEpoch: 0018 loss_train: 1.8135 acc_train: 0.2904 loss_val: 1.8006 acc_val: 0.3015 time: 0.0220s\nEpoch: 0019 loss_train: 1.8638 acc_train: 0.2706 loss_val: 1.7966 acc_val: 0.3015 time: 0.0160s\nEpoch: 0020 loss_train: 1.8239 acc_train: 0.3069 loss_val: 1.7922 acc_val: 0.3015 time: 0.0160s\nEpoch: 0021 loss_train: 1.8112 acc_train: 0.3003 loss_val: 1.7877 acc_val: 0.3015 time: 0.0170s\nEpoch: 0022 loss_train: 1.8339 acc_train: 0.3135 loss_val: 1.7833 acc_val: 0.3015 time: 0.0189s\nEpoch: 0023 loss_train: 1.7961 acc_train: 0.3234 loss_val: 1.7789 acc_val: 0.3015 time: 0.0189s\nEpoch: 0024 loss_train: 1.7938 acc_train: 0.2805 loss_val: 1.7747 acc_val: 0.3015 time: 0.0159s\nEpoch: 0025 loss_train: 1.8386 acc_train: 0.2904 loss_val: 1.7706 acc_val: 0.3015 time: 0.0160s\nEpoch: 0026 loss_train: 1.7684 acc_train: 0.3135 loss_val: 1.7667 acc_val: 0.3015 time: 0.0150s\nEpoch: 0027 loss_train: 1.7786 acc_train: 0.3036 loss_val: 1.7631 acc_val: 0.3015 time: 0.0249s\nEpoch: 0028 loss_train: 1.8269 acc_train: 0.2970 loss_val: 1.7596 acc_val: 0.3015 time: 0.0199s\nEpoch: 0029 loss_train: 1.7972 acc_train: 0.2772 loss_val: 1.7564 acc_val: 0.3015 time: 0.0209s\nEpoch: 0030 loss_train: 1.7905 acc_train: 0.3201 loss_val: 1.7533 acc_val: 0.3015 time: 0.0209s\nEpoch: 0031 loss_train: 1.7494 acc_train: 0.3036 loss_val: 1.7504 acc_val: 0.3015 time: 0.0190s\nEpoch: 0032 loss_train: 1.7679 acc_train: 0.3036 loss_val: 1.7475 acc_val: 0.3015 time: 0.0180s\nEpoch: 0033 loss_train: 1.7422 acc_train: 0.3333 loss_val: 1.7447 acc_val: 0.3015 time: 0.0210s\nEpoch: 0034 loss_train: 1.8057 acc_train: 0.3003 loss_val: 1.7420 acc_val: 0.3015 time: 0.0170s\nEpoch: 0035 loss_train: 1.7633 acc_train: 0.3069 loss_val: 1.7394 acc_val: 0.3015 time: 0.0189s\nEpoch: 0036 loss_train: 1.7312 acc_train: 0.3036 loss_val: 1.7366 acc_val: 0.3015 time: 0.0159s\nEpoch: 0037 loss_train: 1.7599 acc_train: 0.2970 loss_val: 1.7338 acc_val: 0.3015 time: 0.0180s\nEpoch: 0038 loss_train: 1.7509 acc_train: 0.3135 loss_val: 1.7309 acc_val: 0.3015 time: 0.0189s\nEpoch: 0039 loss_train: 1.7337 acc_train: 0.3102 loss_val: 1.7278 acc_val: 0.3015 time: 0.0180s\nEpoch: 0040 loss_train: 1.7296 acc_train: 0.3168 loss_val: 1.7247 acc_val: 0.3015 time: 0.0170s\nEpoch: 0041 loss_train: 1.7159 acc_train: 0.3333 loss_val: 1.7213 acc_val: 0.3015 time: 0.0150s\nEpoch: 0042 loss_train: 1.7319 acc_train: 0.3135 loss_val: 1.7179 acc_val: 0.3015 time: 0.0150s\nEpoch: 0043 loss_train: 1.7213 acc_train: 0.3036 loss_val: 1.7143 acc_val: 0.3015 time: 0.0170s\nEpoch: 0044 loss_train: 1.7207 acc_train: 0.3333 loss_val: 1.7106 acc_val: 0.3015 time: 0.0180s\nEpoch: 0045 loss_train: 1.7060 acc_train: 0.3267 loss_val: 1.7069 acc_val: 0.3015 time: 0.0160s\nEpoch: 0046 loss_train: 1.7026 acc_train: 0.3234 loss_val: 1.7030 acc_val: 0.3015 time: 0.0189s\nEpoch: 0047 loss_train: 1.6783 acc_train: 0.3465 loss_val: 1.6989 acc_val: 0.3031 time: 0.0179s\nEpoch: 0048 loss_train: 1.6832 acc_train: 0.3531 loss_val: 1.6948 acc_val: 0.3031 time: 0.0170s\nEpoch: 0049 loss_train: 1.7033 acc_train: 0.3465 loss_val: 1.6907 acc_val: 0.3046 time: 0.0150s\nEpoch: 0050 loss_train: 1.6633 acc_train: 0.3927 loss_val: 1.6864 acc_val: 0.3062 time: 0.0160s\nEpoch: 0051 loss_train: 1.6861 acc_train: 0.3300 loss_val: 1.6820 acc_val: 0.3062 time: 0.0160s\nEpoch: 0052 loss_train: 1.6683 acc_train: 0.3597 loss_val: 1.6775 acc_val: 0.3092 time: 0.0179s\nEpoch: 0053 loss_train: 1.6749 acc_train: 0.3498 loss_val: 1.6729 acc_val: 0.3092 time: 0.0180s\nEpoch: 0054 loss_train: 1.6335 acc_train: 0.3795 loss_val: 1.6682 acc_val: 0.3092 time: 0.0339s\nEpoch: 0055 loss_train: 1.6693 acc_train: 0.3432 loss_val: 1.6634 acc_val: 0.3108 time: 0.0210s\nEpoch: 0056 loss_train: 1.6454 acc_train: 0.3399 loss_val: 1.6586 acc_val: 0.3169 time: 0.0289s\nEpoch: 0057 loss_train: 1.6314 acc_train: 0.3663 loss_val: 1.6537 acc_val: 0.3200 time: 0.0299s\nEpoch: 0058 loss_train: 1.6679 acc_train: 0.3465 loss_val: 1.6487 acc_val: 0.3262 time: 0.0229s\nEpoch: 0059 loss_train: 1.6039 acc_train: 0.4158 loss_val: 1.6436 acc_val: 0.3292 time: 0.0189s\nEpoch: 0060 loss_train: 1.6219 acc_train: 0.3795 loss_val: 1.6383 acc_val: 0.3308 time: 0.0180s\nEpoch: 0061 loss_train: 1.6243 acc_train: 0.3828 loss_val: 1.6329 acc_val: 0.3354 time: 0.0209s\nEpoch: 0062 loss_train: 1.5949 acc_train: 0.3795 loss_val: 1.6273 acc_val: 0.3369 time: 0.0150s\nEpoch: 0063 loss_train: 1.5902 acc_train: 0.3894 loss_val: 1.6215 acc_val: 0.3446 time: 0.0160s\nEpoch: 0064 loss_train: 1.5827 acc_train: 0.3861 loss_val: 1.6155 acc_val: 0.3492 time: 0.0209s\nEpoch: 0065 loss_train: 1.5880 acc_train: 0.3927 loss_val: 1.6095 acc_val: 0.3554 time: 0.0150s\nEpoch: 0066 loss_train: 1.5750 acc_train: 0.3861 loss_val: 1.6033 acc_val: 0.3615 time: 0.0150s\nEpoch: 0067 loss_train: 1.5704 acc_train: 0.3993 loss_val: 1.5969 acc_val: 0.3677 time: 0.0189s\nEpoch: 0068 loss_train: 1.5679 acc_train: 0.4257 loss_val: 1.5905 acc_val: 0.3738 time: 0.0180s\nEpoch: 0069 loss_train: 1.5611 acc_train: 0.4158 loss_val: 1.5841 acc_val: 0.3800 time: 0.0259s\nEpoch: 0070 loss_train: 1.5657 acc_train: 0.4224 loss_val: 1.5776 acc_val: 0.3877 time: 0.0339s\nEpoch: 0071 loss_train: 1.5795 acc_train: 0.4191 loss_val: 1.5711 acc_val: 0.3923 time: 0.0259s\nEpoch: 0072 loss_train: 1.5129 acc_train: 0.4521 loss_val: 1.5645 acc_val: 0.3985 time: 0.0209s\nEpoch: 0073 loss_train: 1.5630 acc_train: 0.4356 loss_val: 1.5579 acc_val: 0.3985 time: 0.0219s\nEpoch: 0074 loss_train: 1.4862 acc_train: 0.4884 loss_val: 1.5511 acc_val: 0.4000 time: 0.0269s\nEpoch: 0075 loss_train: 1.5311 acc_train: 0.4521 loss_val: 1.5442 acc_val: 0.4031 time: 0.0319s\nEpoch: 0076 loss_train: 1.5425 acc_train: 0.4323 loss_val: 1.5372 acc_val: 0.4123 time: 0.0309s\nEpoch: 0077 loss_train: 1.4997 acc_train: 0.4719 loss_val: 1.5300 acc_val: 0.4246 time: 0.0329s\nEpoch: 0078 loss_train: 1.4789 acc_train: 0.4983 loss_val: 1.5225 acc_val: 0.4246 time: 0.0329s\nEpoch: 0079 loss_train: 1.4763 acc_train: 0.4653 loss_val: 1.5147 acc_val: 0.4338 time: 0.0309s\nEpoch: 0080 loss_train: 1.4871 acc_train: 0.4818 loss_val: 1.5070 acc_val: 0.4446 time: 0.0279s\nEpoch: 0081 loss_train: 1.4562 acc_train: 0.4818 loss_val: 1.4993 acc_val: 0.4508 time: 0.0229s\nEpoch: 0082 loss_train: 1.4450 acc_train: 0.4620 loss_val: 1.4915 acc_val: 0.4477 time: 0.0259s\nEpoch: 0083 loss_train: 1.4626 acc_train: 0.4983 loss_val: 1.4837 acc_val: 0.4538 time: 0.0339s\nEpoch: 0084 loss_train: 1.4366 acc_train: 0.4950 loss_val: 1.4758 acc_val: 0.4631 time: 0.0349s\nEpoch: 0085 loss_train: 1.4300 acc_train: 0.5248 loss_val: 1.4679 acc_val: 0.4662 time: 0.0289s\nEpoch: 0086 loss_train: 1.4507 acc_train: 0.4884 loss_val: 1.4601 acc_val: 0.4692 time: 0.0269s\nEpoch: 0087 loss_train: 1.4179 acc_train: 0.5116 loss_val: 1.4522 acc_val: 0.4754 time: 0.0309s\nEpoch: 0088 loss_train: 1.4123 acc_train: 0.5248 loss_val: 1.4442 acc_val: 0.4785 time: 0.0269s\nEpoch: 0089 loss_train: 1.3972 acc_train: 0.5149 loss_val: 1.4362 acc_val: 0.4815 time: 0.0239s\nEpoch: 0090 loss_train: 1.3878 acc_train: 0.5314 loss_val: 1.4282 acc_val: 0.4862 time: 0.0309s\nEpoch: 0091 loss_train: 1.3600 acc_train: 0.5314 loss_val: 1.4202 acc_val: 0.4985 time: 0.0279s\nEpoch: 0092 loss_train: 1.3599 acc_train: 0.5380 loss_val: 1.4121 acc_val: 0.5046 time: 0.0239s\nEpoch: 0093 loss_train: 1.3523 acc_train: 0.5281 loss_val: 1.4041 acc_val: 0.5215 time: 0.0319s\nEpoch: 0094 loss_train: 1.3815 acc_train: 0.5446 loss_val: 1.3962 acc_val: 0.5277 time: 0.0269s\nEpoch: 0095 loss_train: 1.3443 acc_train: 0.5578 loss_val: 1.3883 acc_val: 0.5338 time: 0.0279s\nEpoch: 0096 loss_train: 1.3632 acc_train: 0.5578 loss_val: 1.3804 acc_val: 0.5400 time: 0.0349s\nEpoch: 0097 loss_train: 1.3327 acc_train: 0.5710 loss_val: 1.3726 acc_val: 0.5477 time: 0.0239s\nEpoch: 0098 loss_train: 1.3286 acc_train: 0.5908 loss_val: 1.3647 acc_val: 0.5492 time: 0.0319s\nEpoch: 0099 loss_train: 1.2847 acc_train: 0.5974 loss_val: 1.3568 acc_val: 0.5569 time: 0.0299s\nEpoch: 0100 loss_train: 1.3162 acc_train: 0.5710 loss_val: 1.3490 acc_val: 0.5615 time: 0.0219s\nEpoch: 0101 loss_train: 1.2833 acc_train: 0.5776 loss_val: 1.3414 acc_val: 0.5692 time: 0.0329s\nEpoch: 0102 loss_train: 1.3070 acc_train: 0.5578 loss_val: 1.3338 acc_val: 0.5754 time: 0.0309s\nEpoch: 0103 loss_train: 1.2703 acc_train: 0.5809 loss_val: 1.3262 acc_val: 0.5831 time: 0.0319s\nEpoch: 0104 loss_train: 1.3012 acc_train: 0.5776 loss_val: 1.3186 acc_val: 0.5877 time: 0.0309s\nEpoch: 0105 loss_train: 1.2539 acc_train: 0.5842 loss_val: 1.3110 acc_val: 0.5923 time: 0.0289s\nEpoch: 0106 loss_train: 1.2581 acc_train: 0.6073 loss_val: 1.3034 acc_val: 0.5954 time: 0.0239s\nEpoch: 0107 loss_train: 1.2462 acc_train: 0.6205 loss_val: 1.2957 acc_val: 0.5985 time: 0.0239s\nEpoch: 0108 loss_train: 1.2199 acc_train: 0.6304 loss_val: 1.2880 acc_val: 0.6015 time: 0.0299s\nEpoch: 0109 loss_train: 1.2489 acc_train: 0.6139 loss_val: 1.2804 acc_val: 0.6077 time: 0.0329s\nEpoch: 0110 loss_train: 1.2009 acc_train: 0.6436 loss_val: 1.2730 acc_val: 0.6123 time: 0.0888s\nEpoch: 0111 loss_train: 1.2235 acc_train: 0.6139 loss_val: 1.2656 acc_val: 0.6138 time: 0.0309s\nEpoch: 0112 loss_train: 1.2220 acc_train: 0.6370 loss_val: 1.2584 acc_val: 0.6138 time: 0.0578s\nEpoch: 0113 loss_train: 1.1680 acc_train: 0.6370 loss_val: 1.2513 acc_val: 0.6138 time: 0.0329s\nEpoch: 0114 loss_train: 1.1958 acc_train: 0.6601 loss_val: 1.2443 acc_val: 0.6154 time: 0.0449s\nEpoch: 0115 loss_train: 1.1826 acc_train: 0.6106 loss_val: 1.2376 acc_val: 0.6169 time: 0.0349s\nEpoch: 0116 loss_train: 1.1636 acc_train: 0.6601 loss_val: 1.2309 acc_val: 0.6215 time: 0.0299s\nEpoch: 0117 loss_train: 1.1679 acc_train: 0.6535 loss_val: 1.2244 acc_val: 0.6231 time: 0.0299s\nEpoch: 0118 loss_train: 1.1948 acc_train: 0.6535 loss_val: 1.2180 acc_val: 0.6262 time: 0.0339s\nEpoch: 0119 loss_train: 1.1407 acc_train: 0.6568 loss_val: 1.2117 acc_val: 0.6308 time: 0.0249s\nEpoch: 0120 loss_train: 1.1362 acc_train: 0.6733 loss_val: 1.2054 acc_val: 0.6338 time: 0.0219s\nEpoch: 0121 loss_train: 1.1274 acc_train: 0.6733 loss_val: 1.1988 acc_val: 0.6338 time: 0.0319s\nEpoch: 0122 loss_train: 1.1403 acc_train: 0.6568 loss_val: 1.1921 acc_val: 0.6354 time: 0.0299s\nEpoch: 0123 loss_train: 1.1361 acc_train: 0.6832 loss_val: 1.1857 acc_val: 0.6369 time: 0.0369s\nEpoch: 0124 loss_train: 1.1062 acc_train: 0.6766 loss_val: 1.1794 acc_val: 0.6369 time: 0.0329s\nEpoch: 0125 loss_train: 1.1125 acc_train: 0.6766 loss_val: 1.1731 acc_val: 0.6369 time: 0.0259s\nEpoch: 0126 loss_train: 1.0583 acc_train: 0.7063 loss_val: 1.1669 acc_val: 0.6385 time: 0.0319s\nEpoch: 0127 loss_train: 1.0955 acc_train: 0.6733 loss_val: 1.1608 acc_val: 0.6385 time: 0.0319s\nEpoch: 0128 loss_train: 1.0801 acc_train: 0.6832 loss_val: 1.1549 acc_val: 0.6431 time: 0.0389s\nEpoch: 0129 loss_train: 1.0784 acc_train: 0.7030 loss_val: 1.1492 acc_val: 0.6431 time: 0.0309s\nEpoch: 0130 loss_train: 1.0779 acc_train: 0.6865 loss_val: 1.1438 acc_val: 0.6431 time: 0.0319s\nEpoch: 0131 loss_train: 1.0652 acc_train: 0.6997 loss_val: 1.1382 acc_val: 0.6431 time: 0.0319s\nEpoch: 0132 loss_train: 1.0699 acc_train: 0.6964 loss_val: 1.1326 acc_val: 0.6446 time: 0.0289s\nEpoch: 0133 loss_train: 1.0609 acc_train: 0.6898 loss_val: 1.1271 acc_val: 0.6462 time: 0.0309s\nEpoch: 0134 loss_train: 1.0414 acc_train: 0.6700 loss_val: 1.1217 acc_val: 0.6462 time: 0.0329s\nEpoch: 0135 loss_train: 1.0333 acc_train: 0.6898 loss_val: 1.1162 acc_val: 0.6462 time: 0.0329s\nEpoch: 0136 loss_train: 1.0427 acc_train: 0.7096 loss_val: 1.1107 acc_val: 0.6462 time: 0.0299s\nEpoch: 0137 loss_train: 1.0422 acc_train: 0.7063 loss_val: 1.1054 acc_val: 0.6477 time: 0.0249s\nEpoch: 0138 loss_train: 1.0509 acc_train: 0.7096 loss_val: 1.1001 acc_val: 0.6508 time: 0.0269s\nEpoch: 0139 loss_train: 0.9999 acc_train: 0.7228 loss_val: 1.0947 acc_val: 0.6538 time: 0.0279s\nEpoch: 0140 loss_train: 0.9899 acc_train: 0.7096 loss_val: 1.0894 acc_val: 0.6569 time: 0.0379s\nEpoch: 0141 loss_train: 0.9959 acc_train: 0.7063 loss_val: 1.0843 acc_val: 0.6585 time: 0.0269s\nEpoch: 0142 loss_train: 0.9912 acc_train: 0.7162 loss_val: 1.0795 acc_val: 0.6585 time: 0.0309s\nEpoch: 0143 loss_train: 0.9837 acc_train: 0.7261 loss_val: 1.0749 acc_val: 0.6677 time: 0.0319s\nEpoch: 0144 loss_train: 1.0077 acc_train: 0.6931 loss_val: 1.0706 acc_val: 0.6754 time: 0.0309s\nEpoch: 0145 loss_train: 1.0114 acc_train: 0.7360 loss_val: 1.0664 acc_val: 0.6785 time: 0.0309s\nEpoch: 0146 loss_train: 0.9875 acc_train: 0.7294 loss_val: 1.0617 acc_val: 0.6800 time: 0.0339s\nEpoch: 0147 loss_train: 0.9509 acc_train: 0.7294 loss_val: 1.0570 acc_val: 0.6831 time: 0.0359s\nEpoch: 0148 loss_train: 0.9585 acc_train: 0.7360 loss_val: 1.0520 acc_val: 0.6846 time: 0.0289s\nEpoch: 0149 loss_train: 0.9678 acc_train: 0.6997 loss_val: 1.0471 acc_val: 0.6923 time: 0.0319s\nEpoch: 0150 loss_train: 0.9938 acc_train: 0.7261 loss_val: 1.0424 acc_val: 0.6938 time: 0.0309s\nEpoch: 0151 loss_train: 0.9760 acc_train: 0.7525 loss_val: 1.0380 acc_val: 0.6938 time: 0.0289s\nEpoch: 0152 loss_train: 0.9534 acc_train: 0.7459 loss_val: 1.0339 acc_val: 0.6908 time: 0.0299s\nEpoch: 0153 loss_train: 0.9890 acc_train: 0.7228 loss_val: 1.0298 acc_val: 0.6923 time: 0.0359s\nEpoch: 0154 loss_train: 0.9501 acc_train: 0.7756 loss_val: 1.0257 acc_val: 0.6954 time: 0.0309s\nEpoch: 0155 loss_train: 0.9443 acc_train: 0.7591 loss_val: 1.0215 acc_val: 0.7000 time: 0.0209s\nEpoch: 0156 loss_train: 0.9466 acc_train: 0.7558 loss_val: 1.0173 acc_val: 0.7077 time: 0.0289s\nEpoch: 0157 loss_train: 0.9299 acc_train: 0.7657 loss_val: 1.0131 acc_val: 0.7108 time: 0.0339s\nEpoch: 0158 loss_train: 0.9380 acc_train: 0.7492 loss_val: 1.0088 acc_val: 0.7169 time: 0.0319s\nEpoch: 0159 loss_train: 0.9170 acc_train: 0.7822 loss_val: 1.0044 acc_val: 0.7262 time: 0.0329s\nEpoch: 0160 loss_train: 0.9068 acc_train: 0.7954 loss_val: 1.0003 acc_val: 0.7292 time: 0.0309s\nEpoch: 0161 loss_train: 0.8922 acc_train: 0.7921 loss_val: 0.9962 acc_val: 0.7292 time: 0.0289s\nEpoch: 0162 loss_train: 0.9112 acc_train: 0.7822 loss_val: 0.9919 acc_val: 0.7277 time: 0.0299s\nEpoch: 0163 loss_train: 0.9139 acc_train: 0.7954 loss_val: 0.9876 acc_val: 0.7292 time: 0.0239s\nEpoch: 0164 loss_train: 0.8933 acc_train: 0.7888 loss_val: 0.9835 acc_val: 0.7277 time: 0.0299s\nEpoch: 0165 loss_train: 0.8805 acc_train: 0.7921 loss_val: 0.9797 acc_val: 0.7323 time: 0.0279s\nEpoch: 0166 loss_train: 0.9080 acc_train: 0.7624 loss_val: 0.9761 acc_val: 0.7323 time: 0.0269s\nEpoch: 0167 loss_train: 0.8762 acc_train: 0.7888 loss_val: 0.9723 acc_val: 0.7308 time: 0.0279s\nEpoch: 0168 loss_train: 0.8509 acc_train: 0.8119 loss_val: 0.9685 acc_val: 0.7338 time: 0.0279s\nEpoch: 0169 loss_train: 0.8891 acc_train: 0.7987 loss_val: 0.9648 acc_val: 0.7323 time: 0.0309s\nEpoch: 0170 loss_train: 0.8969 acc_train: 0.8284 loss_val: 0.9610 acc_val: 0.7385 time: 0.0319s\nEpoch: 0171 loss_train: 0.8629 acc_train: 0.7987 loss_val: 0.9575 acc_val: 0.7431 time: 0.0359s\nEpoch: 0172 loss_train: 0.8830 acc_train: 0.8152 loss_val: 0.9542 acc_val: 0.7446 time: 0.0279s\nEpoch: 0173 loss_train: 0.8847 acc_train: 0.7987 loss_val: 0.9508 acc_val: 0.7492 time: 0.0229s\nEpoch: 0174 loss_train: 0.8712 acc_train: 0.7987 loss_val: 0.9472 acc_val: 0.7508 time: 0.0269s\nEpoch: 0175 loss_train: 0.8407 acc_train: 0.8218 loss_val: 0.9436 acc_val: 0.7523 time: 0.0209s\nEpoch: 0176 loss_train: 0.8712 acc_train: 0.8020 loss_val: 0.9402 acc_val: 0.7554 time: 0.0279s\nEpoch: 0177 loss_train: 0.8329 acc_train: 0.8350 loss_val: 0.9369 acc_val: 0.7585 time: 0.0299s\nEpoch: 0178 loss_train: 0.8201 acc_train: 0.8449 loss_val: 0.9336 acc_val: 0.7569 time: 0.0279s\nEpoch: 0179 loss_train: 0.8214 acc_train: 0.8284 loss_val: 0.9304 acc_val: 0.7600 time: 0.0259s\nEpoch: 0180 loss_train: 0.8241 acc_train: 0.8119 loss_val: 0.9272 acc_val: 0.7615 time: 0.0289s\nEpoch: 0181 loss_train: 0.8251 acc_train: 0.7921 loss_val: 0.9242 acc_val: 0.7615 time: 0.0279s\nEpoch: 0182 loss_train: 0.8232 acc_train: 0.7921 loss_val: 0.9214 acc_val: 0.7615 time: 0.0239s\nEpoch: 0183 loss_train: 0.8081 acc_train: 0.8284 loss_val: 0.9187 acc_val: 0.7646 time: 0.0269s\nEpoch: 0184 loss_train: 0.7993 acc_train: 0.8317 loss_val: 0.9162 acc_val: 0.7723 time: 0.0229s\nEpoch: 0185 loss_train: 0.8190 acc_train: 0.8416 loss_val: 0.9133 acc_val: 0.7785 time: 0.0249s\nEpoch: 0186 loss_train: 0.8473 acc_train: 0.8152 loss_val: 0.9101 acc_val: 0.7800 time: 0.0189s\nEpoch: 0187 loss_train: 0.8230 acc_train: 0.8218 loss_val: 0.9067 acc_val: 0.7815 time: 0.0189s\nEpoch: 0188 loss_train: 0.8355 acc_train: 0.8185 loss_val: 0.9033 acc_val: 0.7785 time: 0.0160s\nEpoch: 0189 loss_train: 0.8104 acc_train: 0.8284 loss_val: 0.9003 acc_val: 0.7769 time: 0.0160s\nEpoch: 0190 loss_train: 0.8432 acc_train: 0.8020 loss_val: 0.8974 acc_val: 0.7800 time: 0.0159s\nEpoch: 0191 loss_train: 0.8124 acc_train: 0.8614 loss_val: 0.8944 acc_val: 0.7738 time: 0.0150s\nEpoch: 0192 loss_train: 0.7382 acc_train: 0.8680 loss_val: 0.8912 acc_val: 0.7754 time: 0.0150s\nEpoch: 0193 loss_train: 0.7815 acc_train: 0.8548 loss_val: 0.8877 acc_val: 0.7754 time: 0.0219s\nEpoch: 0194 loss_train: 0.7997 acc_train: 0.8251 loss_val: 0.8843 acc_val: 0.7831 time: 0.0279s\nEpoch: 0195 loss_train: 0.7648 acc_train: 0.8152 loss_val: 0.8814 acc_val: 0.7892 time: 0.0260s\nEpoch: 0196 loss_train: 0.7647 acc_train: 0.8548 loss_val: 0.8789 acc_val: 0.7985 time: 0.0329s\nEpoch: 0197 loss_train: 0.8344 acc_train: 0.8218 loss_val: 0.8768 acc_val: 0.8000 time: 0.0309s\nEpoch: 0198 loss_train: 0.7488 acc_train: 0.8317 loss_val: 0.8738 acc_val: 0.8031 time: 0.0289s\nEpoch: 0199 loss_train: 0.7927 acc_train: 0.8482 loss_val: 0.8706 acc_val: 0.8000 time: 0.0219s\nEpoch: 0200 loss_train: 0.7407 acc_train: 0.8449 loss_val: 0.8676 acc_val: 0.8015 time: 0.0199s\nTotal time elapsed: 5.8389s\nTest set results: loss= 0.8728 accuracy= 0.8012\n"
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Test model\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}